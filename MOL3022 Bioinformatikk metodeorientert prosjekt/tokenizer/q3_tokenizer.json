"{\"class_name\": \"Tokenizer\", \"config\": {\"num_words\": null, \"filters\": \"!\\\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\", \"lower\": true, \"split\": \" \", \"char_level\": true, \"oov_token\": null, \"document_count\": 88751, \"word_counts\": \"{\\\"c\\\": 3358550, \\\"e\\\": 1618321, \\\"h\\\": 2166432}\", \"word_docs\": \"{\\\"e\\\": 64185, \\\"c\\\": 88751, \\\"h\\\": 74306}\", \"index_docs\": \"{\\\"3\\\": 64185, \\\"1\\\": 88751, \\\"2\\\": 74306}\", \"index_word\": \"{\\\"1\\\": \\\"c\\\", \\\"2\\\": \\\"h\\\", \\\"3\\\": \\\"e\\\"}\", \"word_index\": \"{\\\"c\\\": 1, \\\"h\\\": 2, \\\"e\\\": 3}\"}}"