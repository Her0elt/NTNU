{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.youtube.com/watch?v=ewRw996uevM&list=PLZbbT5o_s2xoWNVdDudn51XM8lOuZ_Njv&index=18\n",
    "#https://www.youtube.com/watch?v=0bt0SjbS3xc&list=PLZbbT5o_s2xoWNVdDudn51XM8lOuZ_Njv&index=13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import  RMSprop\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "MODEL_FILE_NAME = \"dqncartpole.h5\"\n",
    "env = gym.make('CartPole-v0')\n",
    "tf.random.set_seed(200)\n",
    "\n",
    "gpu = len(tf.config.list_physical_devices('GPU')) > 0\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "# tf.debugging.set_log_device_placement(True)\n",
    "tf.test.is_built_with_cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_space = env.observation_space.shape[0] \n",
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_space= env.action_space.n\n",
    "action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNQLearnCartPoleSolver():\n",
    "    def __init__(self, env,  input_shape, action_shape, episodes, epsilon_decay_rate=0.999, min_epsilon=0.001):\n",
    "        self.input_size = input_shape\n",
    "        self.episodes = episodes\n",
    "        self.env = env\n",
    "        self.action_size = action_shape\n",
    "        self.memory = deque([],maxlen=20000)\n",
    "        self.min_epsilon=min_epsilon\n",
    "        self.epsilon_decay_rate = epsilon_decay_rate\n",
    "        self.epsilon = 0.1\n",
    "        self.state_size = input_shape\n",
    "        self.batch_size = 64\n",
    "        self.gamma = 0.99\n",
    "        self.train_start = 1000\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(24, input_dim=input_shape, activation='relu', kernel_initializer='he_uniform'))\n",
    "        self.model.add(Dense(action_shape, activation=\"linear\", kernel_initializer='he_uniform'))\n",
    "\n",
    "        self.model.compile(loss=\"mse\", optimizer=RMSprop(\n",
    "            learning_rate=0.00025), metrics=[\"accuracy\"])\n",
    "\n",
    " \n",
    "\n",
    "    def action(self, state):\n",
    "        # print(f\" rand nr {np.random.random()}  eps {self.epsilon}\")\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            return np.argmax(self.model.predict(state))\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def preprocess_state(self, state):\n",
    "        return np.reshape(state, [1, self.state_size]) \n",
    "  \n",
    "    def update_q_func(self,reward, next_state, done):\n",
    "        if done:\n",
    "            return reward\n",
    "        else:\n",
    "            return reward + self.gamma * np.max(next_state)\n",
    "\n",
    "    def update_q_values(self, minibatch, target, target_next ):\n",
    "        for index, (_, action, reward, _, done) in enumerate(minibatch):\n",
    "            target[index][action] = self.update_q_func(reward, target_next[index], done)\n",
    "\n",
    "    def update_epsilon(self):\n",
    "        self.epsilon *= self.epsilon_decay_rate\n",
    "        self.epsilon = max(self.min_epsilon, self.epsilon)\n",
    "\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, min(len(self.memory), self.batch_size))\n",
    "        states = np.zeros((self.batch_size, self.state_size))\n",
    "        next_states = np.zeros((self.batch_size, self.state_size))\n",
    "        for index, (state, _, _, next_state, _ )in enumerate(minibatch):\n",
    "            states[index] = state\n",
    "            next_states[index] = next_state\n",
    "        target = self.model.predict(states)\n",
    "        target_next = self.model.predict(next_states)\n",
    "        self.update_q_values(minibatch, target, target_next)\n",
    "        self.model.fit(np.array(states), np.array(target), batch_size=self.batch_size, verbose=0)\n",
    "        self.update_epsilon()\n",
    "    \n",
    "\n",
    "    def get_reward(self, done, step, reward):\n",
    "        if not done or step == self.env._max_episode_steps-1:\n",
    "                return reward\n",
    "        else:\n",
    "            return -100\n",
    "    \n",
    "            \n",
    "    def train(self):\n",
    "        scores = []\n",
    "        for episode in range(self.episodes):\n",
    "            done = False\n",
    "            state = self.preprocess_state(self.env.reset())\n",
    "            step = 0\n",
    "            while not done:\n",
    "                # self.env.render()\n",
    "                action = self.action(state)\n",
    "                next_state, reward, done, _ = self.env.step(action) \n",
    "                next_state =  self.preprocess_state(next_state)\n",
    "                step +=1\n",
    "                self.remember(state, action, reward, next_state, done)\n",
    "                reward = self.get_reward(done, step, reward)\n",
    "                state = next_state\n",
    "            scores.append(step)\n",
    "            print(f\"{scores[episode]}  score for ep {episode+1} epsilon {self.epsilon}\")\n",
    "            if step == 200:\n",
    "                print(f\"Saving trained model as {MODEL_FILE_NAME}\")\n",
    "                self.model.save(MODEL_FILE_NAME)\n",
    "            self.replay()\n",
    "        # self.env.close()\n",
    "        print('Finished training!')\n",
    "\n",
    "    def test(self):\n",
    "        self.model = load_model(MODEL_FILE_NAME)\n",
    "        state = self.preprocess_state(self.env.reset())\n",
    "        done = False\n",
    "        score = 0\n",
    "        while not done:\n",
    "            self.env.render()\n",
    "            action = np.argmax(self.model.predict(state))\n",
    "            next_state, reward, done, _ = self.env.step(action)\n",
    "            state = self.preprocess_state(next_state)\n",
    "            score += 1\n",
    "        print(f\"{score}  score\")\n",
    "        self.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89  score for ep 1 epsilon 0.1\n",
      "200  score for ep 2 epsilon 0.1\n",
      "Saving trained model as dqncartpole.h5\n",
      "55  score for ep 3 epsilon 0.1\n",
      "82  score for ep 4 epsilon 0.1\n",
      "200  score for ep 5 epsilon 0.1\n",
      "Saving trained model as dqncartpole.h5\n",
      "141  score for ep 6 epsilon 0.1\n",
      "117  score for ep 7 epsilon 0.1\n",
      "76  score for ep 8 epsilon 0.1\n",
      "59  score for ep 9 epsilon 0.1\n",
      "200  score for ep 10 epsilon 0.0999\n",
      "Saving trained model as dqncartpole.h5\n",
      "200  score for ep 11 epsilon 0.0998001\n",
      "Saving trained model as dqncartpole.h5\n",
      "89  score for ep 12 epsilon 0.0997002999\n",
      "60  score for ep 13 epsilon 0.0996005996001\n",
      "108  score for ep 14 epsilon 0.0995009990004999\n",
      "200  score for ep 15 epsilon 0.0994014980014994\n",
      "Saving trained model as dqncartpole.h5\n",
      "72  score for ep 16 epsilon 0.0993020965034979\n",
      "200  score for ep 17 epsilon 0.09920279440699441\n",
      "Saving trained model as dqncartpole.h5\n",
      "101  score for ep 18 epsilon 0.09910359161258742\n",
      "200  score for ep 19 epsilon 0.09900448802097483\n",
      "Saving trained model as dqncartpole.h5\n",
      "53  score for ep 20 epsilon 0.09890548353295386\n",
      "98  score for ep 21 epsilon 0.0988065780494209\n",
      "146  score for ep 22 epsilon 0.09870777147137148\n",
      "200  score for ep 23 epsilon 0.0986090636999001\n",
      "Saving trained model as dqncartpole.h5\n",
      "81  score for ep 24 epsilon 0.0985104546362002\n",
      "93  score for ep 25 epsilon 0.098411944181564\n",
      "108  score for ep 26 epsilon 0.09831353223738244\n",
      "80  score for ep 27 epsilon 0.09821521870514506\n",
      "113  score for ep 28 epsilon 0.09811700348643991\n",
      "91  score for ep 29 epsilon 0.09801888648295347\n",
      "80  score for ep 30 epsilon 0.09792086759647052\n",
      "200  score for ep 31 epsilon 0.09782294672887405\n",
      "Saving trained model as dqncartpole.h5\n",
      "165  score for ep 32 epsilon 0.09772512378214518\n",
      "76  score for ep 33 epsilon 0.09762739865836303\n",
      "200  score for ep 34 epsilon 0.09752977125970468\n",
      "Saving trained model as dqncartpole.h5\n",
      "70  score for ep 35 epsilon 0.09743224148844497\n",
      "99  score for ep 36 epsilon 0.09733480924695652\n",
      "122  score for ep 37 epsilon 0.09723747443770957\n",
      "62  score for ep 38 epsilon 0.09714023696327186\n",
      "90  score for ep 39 epsilon 0.09704309672630858\n",
      "200  score for ep 40 epsilon 0.09694605362958227\n",
      "Saving trained model as dqncartpole.h5\n",
      "114  score for ep 41 epsilon 0.09684910757595268\n",
      "200  score for ep 42 epsilon 0.09675225846837672\n",
      "Saving trained model as dqncartpole.h5\n",
      "119  score for ep 43 epsilon 0.09665550620990834\n",
      "93  score for ep 44 epsilon 0.09655885070369843\n",
      "116  score for ep 45 epsilon 0.09646229185299474\n",
      "200  score for ep 46 epsilon 0.09636582956114174\n",
      "Saving trained model as dqncartpole.h5\n",
      "70  score for ep 47 epsilon 0.0962694637315806\n",
      "200  score for ep 48 epsilon 0.09617319426784902\n",
      "Saving trained model as dqncartpole.h5\n",
      "75  score for ep 49 epsilon 0.09607702107358118\n",
      "81  score for ep 50 epsilon 0.0959809440525076\n",
      "164  score for ep 51 epsilon 0.09588496310845508\n",
      "100  score for ep 52 epsilon 0.09578907814534662\n",
      "48  score for ep 53 epsilon 0.09569328906720127\n",
      "200  score for ep 54 epsilon 0.09559759577813408\n",
      "Saving trained model as dqncartpole.h5\n",
      "200  score for ep 55 epsilon 0.09550199818235594\n",
      "Saving trained model as dqncartpole.h5\n",
      "181  score for ep 56 epsilon 0.09540649618417359\n",
      "85  score for ep 57 epsilon 0.09531108968798942\n",
      "65  score for ep 58 epsilon 0.09521577859830142\n",
      "99  score for ep 59 epsilon 0.09512056281970312\n",
      "93  score for ep 60 epsilon 0.09502544225688342\n",
      "98  score for ep 61 epsilon 0.09493041681462654\n",
      "97  score for ep 62 epsilon 0.0948354863978119\n",
      "84  score for ep 63 epsilon 0.09474065091141409\n",
      "200  score for ep 64 epsilon 0.09464591026050267\n",
      "Saving trained model as dqncartpole.h5\n",
      "120  score for ep 65 epsilon 0.09455126435024218\n",
      "70  score for ep 66 epsilon 0.09445671308589193\n",
      "101  score for ep 67 epsilon 0.09436225637280604\n",
      "200  score for ep 68 epsilon 0.09426789411643323\n",
      "Saving trained model as dqncartpole.h5\n",
      "114  score for ep 69 epsilon 0.0941736262223168\n",
      "200  score for ep 70 epsilon 0.09407945259609449\n",
      "Saving trained model as dqncartpole.h5\n",
      "81  score for ep 71 epsilon 0.09398537314349839\n",
      "200  score for ep 72 epsilon 0.0938913877703549\n",
      "Saving trained model as dqncartpole.h5\n",
      "80  score for ep 73 epsilon 0.09379749638258454\n",
      "83  score for ep 74 epsilon 0.09370369888620196\n",
      "107  score for ep 75 epsilon 0.09360999518731576\n",
      "192  score for ep 76 epsilon 0.09351638519212845\n",
      "110  score for ep 77 epsilon 0.09342286880693632\n",
      "71  score for ep 78 epsilon 0.09332944593812938\n",
      "200  score for ep 79 epsilon 0.09323611649219125\n",
      "Saving trained model as dqncartpole.h5\n",
      "134  score for ep 80 epsilon 0.09314288037569907\n",
      "69  score for ep 81 epsilon 0.09304973749532337\n",
      "200  score for ep 82 epsilon 0.09295668775782805\n",
      "Saving trained model as dqncartpole.h5\n",
      "50  score for ep 83 epsilon 0.09286373107007022\n",
      "86  score for ep 84 epsilon 0.09277086733900015\n",
      "166  score for ep 85 epsilon 0.09267809647166116\n",
      "69  score for ep 86 epsilon 0.0925854183751895\n",
      "56  score for ep 87 epsilon 0.0924928329568143\n",
      "112  score for ep 88 epsilon 0.09240034012385749\n",
      "96  score for ep 89 epsilon 0.09230793978373364\n",
      "160  score for ep 90 epsilon 0.0922156318439499\n",
      "86  score for ep 91 epsilon 0.09212341621210596\n",
      "107  score for ep 92 epsilon 0.09203129279589385\n",
      "81  score for ep 93 epsilon 0.09193926150309796\n",
      "200  score for ep 94 epsilon 0.09184732224159486\n",
      "Saving trained model as dqncartpole.h5\n",
      "87  score for ep 95 epsilon 0.09175547491935326\n",
      "190  score for ep 96 epsilon 0.09166371944443391\n",
      "200  score for ep 97 epsilon 0.09157205572498947\n",
      "Saving trained model as dqncartpole.h5\n",
      "200  score for ep 98 epsilon 0.09148048366926448\n",
      "Saving trained model as dqncartpole.h5\n",
      "119  score for ep 99 epsilon 0.09138900318559522\n",
      "116  score for ep 100 epsilon 0.09129761418240961\n",
      "Finished training!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = DQNQLearnCartPoleSolver(env, state_space, action_space, episodes=100)\n",
    "model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200  score\n"
     ]
    }
   ],
   "source": [
    "model.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation\n",
    "\n",
    "## Model\n",
    "```python\n",
    "self.model = Sequential()\n",
    "self.model.add(Dense(24, input_dim=input_shape, activation='relu',kernel_initializer='he_uniform'))\n",
    "self.model.add(Dense(action_shape, activation=\"linear\", kernel_initializer='he_uniform'))\n",
    "```\n",
    "\n",
    "We start with an input layer of the size of the observation space of the enviorment as we can see at the top of the file it is 4, then comes the  neural network part of the model which are all the dense layers, which creates hidden layers with n nodes. Every Hidden layer is wraped with a relu activation function which simplifies the data in the network, this is done by applying a max function on the value and 0 which leads to only positive values. Every layer also has a kernel initializer set to he uniform which initializes all the weights to non zero values in the different layers. More spesificly it draws samples from a uniform distribution within [-limit, limit] where limit is sqrt(6 / fan_in) where fan_in is the number of input units in the weight tensor. The last layer has activation linear to shape the output of the model. This model has only one hidden layer because the cartpole problem is a pretty easy problem to solve.\n",
    "\n",
    "## Optimizer\n",
    "```python\n",
    "self.model.compile(loss=\"mse\", optimizer=RMSprop(\n",
    "            learning_rate=0.00025, epsilon=0.01), metrics=[\"accuracy\"])\n",
    "```\n",
    "\n",
    "Since DQN is a RNN the RMSprop optimizer is used, which beats out normal gradient decent and adam optimizers for RNN, read more here: [Optimizers](https://ruder.io/optimizing-gradient-descent/index.html)\n",
    "\n",
    "## Code\n",
    "Most of the logic here is the same as in QLearn-cartpole.ipynb\n",
    "\n",
    "When training we firstly loop for n episodes given to the model on creation. For each episode we reset the env as the init state\n",
    " ```python  \n",
    " self.preprocess_state(self.env.reset())\n",
    "\n",
    " def preprocess_state(self, state):\n",
    "        return np.reshape(state, [1, self.state_size]) \n",
    "  \n",
    " ```\n",
    "\n",
    "\n",
    "state is always transformed into and (1, n) array so it can be used as an input to the model. for every episode we loop until the agent has either failed by tiping or won by getting 200 points. we then choose an action, and preform that action.\n",
    "```python\n",
    "action = self.action(state)\n",
    "next_state, reward, done, _ = self.env.step(action)\n",
    "\n",
    "\n",
    " def action(self, state):\n",
    "    if np.random.random() <= self.epsilon:\n",
    "        return random.randrange(self.action_size)\n",
    "    else:\n",
    "        return np.argmax(self.model.predict(state))\n",
    "```\n",
    "when choosing an action we either explore or exploit this is chosen by generating a random number between 0, 1 and then comparing it to epsilon. if the number is smaller than epsilon we use the model to predict a action (exploit) same as in normal q-learning if the number is lager than epsilon we choose a random action in the env.\n",
    "\n",
    "\n",
    "In normal q-learn we would now update the q table with new q values by Optimizing the q function, but in DQN we instead push the current state, action, reward, next state, and if the agent is done or not. Since we dont update q-values in a DQN we instead use the stored values to \"replay\" the previous attempt and train on the model on the values stored in replay memory which is where we put all the values from each step during the while loop. This is done by firstly picking out a subset of values from the replay memory\n",
    "```python\n",
    "minibatch = random.sample(self.memory, min(len(self.memory), self.batch_size))\n",
    "```\n",
    "we then use the model to predict q values for the current state and the next state\n",
    "\n",
    "```python\n",
    "target = self.model.predict(states)\n",
    "target_next = self.model.predict(next_states)\n",
    "```\n",
    "we then update the target values which we are going to use for training with the predicted next\n",
    "\n",
    "```python\n",
    "def update_q_func(self,reward, next_state, done):\n",
    "    if done:\n",
    "        return reward\n",
    "    else:\n",
    "        return reward + self.gamma * np.max(next_state)\n",
    "\n",
    "def update_q_values(self, minibatch, target, target_next ):\n",
    "    for index, (_, action, reward, _, done) in enumerate(minibatch):\n",
    "        target[index][action] = self.update_q_func(reward, target_next[index], done)\n",
    "```\n",
    "\n",
    "then we train the model with the the states from the batch and the targets\n",
    "```python\n",
    "self.model.fit(np.array(states), np.array(target), batch_size=self.batch_size, verbose=0)\n",
    "```\n",
    "we then update the epsilon value to reduce it since the model is now better so we should trust it more and exploit more then we explore.\n",
    "\n",
    "```python\n",
    "def update_epsilon(self):\n",
    "    if self.epsilon > self.min_epsilon:\n",
    "        self.epsilon *= self.epsilon_decay_rate\n",
    "```\n",
    "\n",
    "## Side notes\n",
    "With a lower epsilon the DQN seems to preform better faster, this might be because it results in less exploration and more exploitation which might in this case be good.\n",
    "\n",
    "\"I think the problem is with openAI gym CartPole-v0 environment reward structure. The reward is always +1 for each time step. So if pole falls reward is +1 itself. So we need to check and redefine the reward for this case. So in the train function try this:\"\n",
    "\n",
    "```python\n",
    "if not done:\n",
    "    new_q = reward + DISCOUNT * np.max(future_qs_list)\n",
    "else:\n",
    "    # if done assign some negative reward\n",
    "    new_q = -20\n",
    "```\n",
    "[Source](https://ai.stackexchange.com/questions/22986/my-deep-q-learning-network-does-not-learn-for-openai-gyms-cartpole-problem)\n",
    "\n",
    "this might not me necessary \n",
    "\n",
    "       "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "98ea75fd2e5ecfe211fd1140bc1e49d2d22e4f681cda7a5605556243f3afc82f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('tf-gpu': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
